TODO:

1) stream by location.                                                                                              DONE
    figure out API
    Can just pass in 4 location points in a list.
        Twitter wants a comma seperated list of latlong points.
        Twitter wants southwest corner coming first
        NOTE: google maps has long lat. We want lat long. (reverse them)


        SOUTHWEST US: (-124, 30)
        NORTHEAST US: ( -61, 49)

        THUS locations = [-124, 30, -61, 49]

        Perhaps specify for 50 different states. Just to get more datapoints.
2) 50 different states. Make seperate accounts for each.
3) figure out how to auto-restart script if something breaks it.                                                    DONE
    plan is:
        1) if twitter connection error (something for which twitter gives error code), then fix issue in code.
            need to figure out a way to stop the streaming for a while and then restart it up again.
                IDEAS:
                    1) just have timer wait for a few min.
           WAIT. I THINK that Streaming api already handles automatically sleeping and then restarting!
           WHY? There are a bunch of "sleep" and "retry_time_start" in the streaming.py code
        2) if python script crashes, then manually rerun it.
            There is some very BIG problem that needs to be fixed.
        3) email in BOTH cases.
    MAYBE DONE. NEED TO CHECK.
3) handle status_codes.                                                                                             DONE
    only 200, 420, ... NOT very useful
4) handle ERROR codes                                                                                               DONE
    more useful. listed at bottom of get_data.py. how to access them?
    error codes sent whenever the stream fails
    USEFUL ERROR CODES:
        1	Shutdown	The feed was shutdown (possibly a machine restart)     -> restart
        2	Duplicate stream	The same endpoint was connected too many times.   -> stop
        4	Stall	The client was reading too slowly and was disconnected by the server. -> restart
        5	Normal	The client appeared to have initiated a disconnect.                   -> restart
        7	Admin logout   same credentials used to connect to new stream and oldest disconnected.  -> restart
        10	Stream exception	An internal issue disconnected the stream.   -> restart
        11	Broker stall	An internal issue disconnected the stream.     -> restart
        12	Shed load	The host the stream was connected to became overloaded. -> restart

        THUS, just restart if error is thrown. If duplicate stream error, then just stop stream.
            restart should be happening automatically. really need to check.
                    and actually the 420 error code RATE LIMITING handles this already...

5) store data in db or whatever store                                                                               DONE
    figure out what db we all want to store data in.
    figure out schema
6) run periodically (cron job or continuously running python script? )
    almost done.
    Need to find a way to run python script and have it continuously running.
    screen
        screen will start a new 'screen'
        ctrl-A D will detach your 'screen'
            can leave shell and log out of server.
        screen -r will reatach to your 'screen'
7) send email if there is an issue so we can figure out whats wrong.                                                DONE
8) perhaps only get ascii values?
9) https://dev.twitter.com/streaming/overview/messages-types#disconnect_messages
    handle diff types of disconnect messages.
    other stuff on this page.
10) enable stall warnings.                                                                                          DONE
    These help tell you when to cut down connection due to potential future issues
    1)figured out how to have tweepy use stall_warnings.
11) figure out how much data is actually being sent to me.                                                          DONE
    https://twittercommunity.com/t/is-there-a-limit-to-the-amount-of-data-the-streaming-api-will-send-out/8482/5
    ---Twitter sends me MAX of ~1% of all data that it gets per twitter-second.
        twitter gets 500 million per day -> 5787 per sec.
    ---{'limit': {'track': 13}} reveals that I hit the max rate.
          13 is the number of tweets that were NOT sent to me at that twitter-second.
    ---I am only asking for tweets that have been properly geotagged thus I am NOT getting all tweets.
    ---how many tweets am I getting in a normal say 5 sec time period???
            800 in 30 sec -> 26/sec -> 2 million per day -> 200 million in 100 days.
            26 / 5787 = .05% which is actually pretty good considering that the stat is .1% for the world.

    ON UVA SERVER
    ---how many tweets am I getting in a normal say 5 sec time period???
            640 in 30 sec -> 21/sec -> 1.8 million per day -> 180 million in 100 days.
            21 / 5787 = .036% which is not so good considering that the stat is .1% for the world.



    MEANING we need to set up multiple get_data.py's.

